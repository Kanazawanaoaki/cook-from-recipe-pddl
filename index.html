<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- TODO change title -->
    <title>
      Real-World Cooking Robot System from Recipes Based on Food State Recognition Using Foundation Models and PDDL
    </title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.7.0/font/bootstrap-icons.css" rel="stylesheet">
    <link rel="stylesheet" href="assets/css/main.css">

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-6Y6SNC0HGF"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-6Y6SNC0HGF');
    </script>

    <meta property="og:url"           content="https://kanazawanaoaki.github.io/cook-from-recipe-pddl" />
    <meta property="og:type"          content="website" />
    <meta property="og:title"         content="
    Real-World Cooking Robot System from Recipes Based on Food State Recognition Using Foundation Models and PDDL
    " />
    <meta property="og:description"   content="
                                               Although there is a growing demand for cooking behaviours as one of the expected tasks for robots, a series of cooking behaviours based on new recipe descriptions by robots in the real world has not yet been realised.
                                               In this study, we propose a robot system that integrates real-world executable robot cooking behaviour planning using the Large Language Model (LLM) and classical planning of PDDL descriptions, and food ingredient state recognition learning from a small number of data using the Vision-Language model (VLM).
                                               We succeeded in experiments in which PR2, a dual-armed wheeled robot, performed cooking from arranged new recipes in a real-world environment, and confirmed the effectiveness of the proposed system.
                                               " />
    <meta property="og:image" content="https://kanazawanaoaki.github.io/cook-from-recipe-pddl/assets/img/ar-2023-cook-recipe-thumbnail.png" />
  </head>
  <body>
    <div class="container-fluid">
      <div class="row">
        <div class="col-lg-8 offset-lg-2 col-md-12">

          <div class="text-center">
            <h1 class="mt-5"><b>Real-World Cooking Robot System from Recipes</b></h1>
            <h2 class="mt-3"><b>Based on Food State Recognition Using Foundation Models and PDDL</b></h1>
            <h4 class="mt-4 conf"><b>Advanced Robotics</b></h4>
            <ul class="list-inline mt-4">
              <li class="list-inline-item"><a href="https://kanazawanaoaki.github.io" target="_blank">Naoaki Kanazawa</a></li>
              <li class="list-inline-item ml-4">Kento Kawaharazuka</li>
              <li class="list-inline-item ml-4">Yoshiki Obinata</li>
              <li class="list-inline-item ml-4">Kei Okada</li>
              <li class="list-inline-item ml-4">Masayuki Inaba</li>
              <li class="mt-2">
                JSK Robotics Laboratory, The University of Tokyo, Japan
              </li>
            </ul>
            <ul class="list-inline mt-4">
              <li class="list-inline-item">
                <a href="https://www.tandfonline.com/doi/full/10.1080/01691864.2024.2407136" target="_blank">Paper</a>
              </li>
              <li class="list-inline-item ml-4">
                <a href="http://arxiv.org/" target="_blank">Arxiv</a>
              </li>
              <li class="list-inline-item ml-4">
                <a href="https://youtu.be/3bQRTAKV0wM" target="_blank">Video</a>
              </li>
            </ul>
          </div>

          <div class="row mt-4">
          <div class="col-lg-10 offset-lg-1">
            <p>
            Although there is a growing demand for cooking behaviours as one of the expected tasks for robots, a series of cooking behaviours based on new recipe descriptions by robots in the real world has not yet been realised.
            In this study, we propose a robot system that integrates real-world executable robot cooking behaviour planning using the Large Language Model (LLM) and classical planning of PDDL descriptions, and food ingredient state recognition learning from a small number of data using the Vision-Language model (VLM).
            We succeeded in experiments in which PR2, a dual-armed wheeled robot, performed cooking from arranged new recipes in a real-world environment, and confirmed the effectiveness of the proposed system.
            </p>
          </div>
          </div>

          <!-- overview -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Cooking Robot System from Recipes</h4>
            <p>
              Real-world cooking robot system considering food state changes from recipe descriptions using foundation models and classical planning PDDL.
              The input recipe description is converted into the function sequence by the Large Language Model (LLM), and the executable action procedure is planned by classical planning of PDDL description from the sequence. The robot performs cooking actions while recognizing the state change of the ingredients by food state recognition learning from small data using the Vision-Language Model (VLM). Motion execution is performed using predefined action trajectories.

            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <!-- <img src="assets/img/ar2023-recipe-system-concept.png" class="img-fluid" style="width: 600px; height: auto;"> -->
                <img src="assets/img/ar2023-recipe-system-concept.png" class="img-fluid"">
              </div>
            </div>
          </div>

          <!-- planning -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Real-World Executable Robotic Cooking Action Planning from Recipe</h4>
            <p>
              First, the input recipe description in natural language is converted into the cooking function sequence that can be interpreted by the robot using the few-shot prompting of Large Language Model (LLM).
              The black text in the figure shows the actual prompts, and its last recipe section depends on the natural language description of the recipe to be converted. The blue part is the result of the conversion that the LLM outputs.
              Next, rule-based processing transforms the cooking function sequence into corresponding target conditions for each step within the PDDL description.
              Finally, classical symbolic planning using the PDDL description is used to plan the complementary action steps so that they can be executed in the real environment.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/ar2023-recipe-plan-flow.png" class="img-fluid" style="width: 700px; height: auto;">
              </div>
            </div>
          </div>

          <!-- Recognition -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Food State Recognition Learning from Small Data using Vision-Language Model</h4>
            <p>
              We learn to recognize food state from a small number of data using CLIP's linear-probe for the image input of the gazing area.
              During inference, the learned model is used to infer the state of the foodstuff in real time, and the time when the inferred state first becomes the label after the state change is used as the timing of the state change.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/ar2023-recipe-vlm-rec-flow.png" class="img-fluid" style="width: 400px; height: auto;">
              </div>
            </div>
          </div>

          <!-- Experiments -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4>Real-World Experiments with Robot Cooking Execution from Recipes</h4>
            <p>
              The proposed system plans cooking actions that can be executed in the real world based on natural language recipe descriptions, and executes them in sequence while recognizing changes in the state of food ingredients during heating using the Vision-Language Model. In this experiment, the robot performed the motions that were created by a human by using direct teach, etc.
            </p>
            <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
            <h5>Butter arranged sunny-up</h5>
            <p>
              A new recipe for sunny-side up, arranged to use butter instead of oil, was cooked by the robot in the real world using the proposed system.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/ar2023-recipe-planed-unknown-butter.png" class="img-fluid" style="width: 700px; height: auto;">
              </div>
            </div>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/movie-ar-butter-sunny.gif" class="img-fluid">
              </div>
            </div>
            <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
            <h5>Boiled and sauteed broccoli</h5>
            <p>
              The cooking was executed in the same way for the unknown recipe of boiled and sauteed broccoli.
            </p>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/ar2023-recipe-planed-unknown-broccoli.png" class="img-fluid" style="width: 700px; height: auto;">
              </div>
            </div>
            <div align="center" class="row mt-4">
              <div class="col-lg-10 offset-lg-1">
                <img src="assets/img/movie-ar-broccoli.gif" class="img-fluid">
              </div>
            </div>
          </div>

          <!-- bibtex -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1">
            <h4 id="bibtex">Bibtex</h4>
            <pre>
@article{kanazawa2024recipe,
  author={Naoaki Kanazawa, Kento Kawaharazuka, Yoshiki Obinata, Kei Okada and Masayuki Inaba},
  title={{Real-World Cooking Robot System from Recipes Based on Food State Recognition Using Foundation Models and PDDL}},
  journal={Advanced Robotics},
  pages={1--17},
  year={2024},
  publisher={Taylor \& Francis}
}
            </pre>
          </div>

          <!-- contact -->
          <div class="col-lg-10 offset-lg-1"><hr class="mt-4 mb-4"></div>
          <div class="col-lg-10 offset-lg-1 mb-5">
            <h4 id="contact">Contact</h4>
            <p>
            If you have any questions, please feel free to contact
            <a href="https://kanazawanaoaki.github.io" target="_blank">Naoaki Kanazawa</a>.
            </p>
          </div>

        </div>
      </div>
    </div>
  </body>
</html>
